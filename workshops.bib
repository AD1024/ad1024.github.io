@article{hle2026,
	abstract = {Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve more than 90{\%} accuracy on popular benchmarks such as Measuring Massive Multitask Language Understanding1, limiting informed measurement of state-of-the-art LLM capabilities. Here, in response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be an expert-level closed-ended academic benchmark with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable but cannot be quickly answered by internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a marked gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.},
	author = {Center for AI Safety and Scale AI and HLE Contributors Consortium},
	date = {2026/01/01},
	date-added = {2026-01-30 08:32:32 -0500},
	date-modified = {2026-01-30 08:54:45 -0500},
	doi = {10.1038/s41586-025-09962-4},
	id = {Phan2026},
	isbn = {1476-4687},
	journal = {Nature},
	number = {8099},
    img = {assets/img/img-hle.png},
    booktitle = {Nature},
    html = {https://lastexam.ai/},
    pdf = {https://doi.org/10.1038/s41586-025-09962-4},
    code = {https://github.com/centerforaisafety/hle},
	pages = {1139--1146},
	title = {A benchmark of expert-level academic questions to assess AI capabilities},
	url = {https://doi.org/10.1038/s41586-025-09962-4},
	volume = {649},
	year = {2026},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-025-09962-4}}

@inproceedings{he2025lmpl,
    author = {He, Mike and Ang, Zhendong and Desai, Ankush and Gupta, Aarti},
    title = {Ranking Formal Specifications using LLMs},
    year = {2025},
    isbn = {9798400721489},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3759425.3763386},
    doi = {10.1145/3759425.3763386},
    abstract = {Formal specifications are essential for reasoning about the correctness of complex systems. While recent advances have explored automatically learning such specifications, the challenge of distinguishing meaningful, non-trivial specifications from a vast and noisy pool of learned candidates remains largely open. In this position paper, we present an approach for specification ranking, aimed at identifying the most critical specifications that contribute to overall system correctness. To this end, we develop a four-metric rating framework that quantifies the importance of a specification. Our approach leverages the reasoning capabilities of Large Language Models to rank specifications from a set of automatically learned candidates. We evaluate the proposed method on a set of specifications inferred for 11 open-source and 3 proprietary distributed system benchmarks, demonstrating its effectiveness in ranking critical specifications.},
    booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages},
    pages = {51â€“56},
    numpages = {6},
    keywords = {Agentic workflows, Ranking specifications},
    location = {Singapore, Singapore},
    series = {LMPL '25},
    html = {https://dl.acm.org/doi/10.1145/3759425.3763386},
    code = {https://github.com/p-org/P/tree/experimental/pinfer/Src/PInfer/Scripts},
    pdf = {https://dl.acm.org/doi/pdf/10.1145/3759425.3763386},
    img = {assets/img/img-lmpl-2025.png}
}

@article {he23extraction,
	title = {Improving Term Extraction with Acyclic Constraints},
	booktitle = {E-Graph Research, Applications, Practices, and Human-factors Symposium (PLDI/EGRAPHS'23)},
	year = {2023},
	url = {},
    img = {assets/img/img-egraphs23.png},
    html = {https://pldi23.sigplan.org/details/egraphs-2023-papers/3/Improving-Term-Extraction-with-Acyclic-Constraints},
    pdf = {https://www.cs.princeton.edu/~dh7120/assets/papers/EGRAPHS2023.pdf},
    code = {https://github.com/AD1024/flexmatch/blob/egraphs2023/flexmatch/src/maxsat_extract.rs},
	author = {Mike He and Haichen Dong and Sharad Malik and Aarti Gupta}}

@article {huang21latte,
	title = {From DSLs to Accelerator-rich Platform Implementations: Addressing the Mapping Gap},
	booktitle = {Workshop on Languages, Tools, and Techniques for Accelerator Design (LATTE'21)},
	year = {2021},
	url = {https://capra.cs.cornell.edu/latte21/paper/30.pdf},
	author = {Bo-Yuan Huang* and Steven Lyubomirsky* and Thierry Tambe* and Li, Yi and Mike He and Gus Smith and Gu-Yeon Wei and Aarti Gupta and Sharad Malik and Zachary Tatlock},
    html = {https://vlsiarch.eecs.harvard.edu/publications/dsls-accelerator-rich-platform-implementations-addressing-mapping-gap},
    poster = {https://vlsiarch.eecs.harvard.edu/sites/hwpi.harvard.edu/files/vlsiarch/files/slides.pdf?m=1651755768},
    pdf = {https://vlsiarch.eecs.harvard.edu/publications/dsls-accelerator-rich-platform-implementations-addressing-mapping-gap},
    img = {assets/img/img-latte.png}
}
